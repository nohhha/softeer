### 리뷰

- W5M1을 진행할 때 한 달의 데이터만 가지고 rdd로 불러왔더니 dataframe에 비해서 장점을 크게 느끼지 못했다. 그 이유를 생각해봤을 때 RDD는 파티셔닝과 메모리 사용에 장점을 가지고 있는데 아직 그만큼의 큰 데이터를 사용하지 않아서 장점을 느끼지 못한 것 같다.
- Parquet 바이너리 포맷은 sc.textFile() 로는 깨진 문자열이 출력되므로 DataFrame API(spark.read.parquet(…))로 읽은 뒤 .rdd 로 변환했다.
- reduce와 reducebykey 함수가 헷갈렸는데 reduce는 RDD 전체를 하나의 값으로 축소 (예: 총매출)이고 reducebykey는 키별 집계 (예: 날짜별 건수, 날짜별 매출)라서 서로 다른 상황에 쓰이는 것을 알게 되었다.
- 처음에는 groupBy/groupByKey+collect() 을 사용했는데 이는 파티션 셔플 후 전체 레코드를 Driver로 끌어와서 비효율적이었다. 따라서 reduceByKey, aggregateByKey, countByValue 등 키-값 집계 전용 API 사용해서 전체 레코드를 가져오지 않게 했다
- spark UI를 통해 DAG 구조를 확인하고, spark가 알아서 필요없는 연산은 스킵하는 것을 보고 신기했다…

### 회고

- keep
- problem
    - 급한 마음에 rdd에 사용되는 함수가 정확히 어떻게 사용되는지 이해하지 않고 바로 사용했더니 오류가 누적해서 발생했다. 함수의 인풋과 아웃풋이 어떻게 나오는지 이해하고 사용할 것이다.
- try
